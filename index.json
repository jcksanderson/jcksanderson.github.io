[{"content":"Hi! My name is Jack and I\u0026rsquo;m a third-year undergrad at the University of Chicago studying statistics and computer science. I\u0026rsquo;m largely interested in AI security, including model tampering, data poisoning, and automated adaptive attacks. This summer I created an AI Security Course along with some other undergrads at UChicago. I\u0026rsquo;m also generally involved with AI safety (although I reject some of the field\u0026rsquo;s \u0026ldquo;classical\u0026rdquo; views). In the past, I investigated compute-algorithm interactions, which was featured on Epoch AI\u0026rsquo;s Gradient Updates.\n","date":"January 24, 2026","externalUrl":null,"permalink":"/","section":"Jack Sanderson","summary":"","title":"Jack Sanderson","type":"page"},{"content":"There\u0026rsquo;s no overarching theme here, just various things I think about.\n","date":"January 24, 2026","externalUrl":null,"permalink":"/posts/","section":"posts","summary":"","title":"posts","type":"posts"},{"content":" There are a lot of hypotheses out there relating to machine learning concepts. These are quite hard (maybe impossible) to formally prove, but most of them are generally accepted as true, at least under certain conditions. I thought it would be fun to collect some of my favorites in one place; please let me know if I\u0026rsquo;m missing any that you feel are important!\nCanonical Hypotheses # The Linear Representation Hypothesis # The linear representation hypothesis (LRH) is technically a number of hypotheses, but put simply, it\u0026rsquo;s the idea that LLMs represent \u0026ldquo;features\u0026rdquo; or \u0026ldquo;concepts\u0026rdquo; linearly in their latent spaces. Anything related to linear probing, steering vectors, or a something being represented by a \u0026ldquo;single direction\u0026rdquo; is related to the LRH. Recommended paper: The Linear Representation Hypothesis and the Geometry of Large Language Models. (I\u0026rsquo;ve also posted previously on the math behind why this is possible!)\nThe Lottery Ticket Hypothesis # The lottery ticket hypothesis (LTH), as phrased by the original paper, states that\nA randomly-initialized, dense neural network contains a subnetwork that is initialized such that—when trained in isolation—it can match the test accuracy of the original network after training for at most the same number of iterations.\nWe can think of this subnetwork as a \u0026ldquo;winning ticket\u0026rdquo;. The LTH suggests that the reason why neural networks with tons of parameters perform well is not because all the parameters are necessary, but because having so many parameters greatly increases the \u0026ldquo;search space\u0026rdquo; for winning ticket subnetworks.\nThe Manifold Hypothesis # The manifold hypothesis (MH) states that that most real-world high-dimensional data lives in a much lower-dimensional manifold of the original high-dimensional space.1 In theory, the MH helps explain the efficacy of machine learning techniques on real world data; they only have to model these lower-dimensional subspaces, rather than needing to \u0026ldquo;understand\u0026rdquo; the full high-dimensional space. The MH was very relevant in early neural scaling laws (see, e.g., A Neural Scaling Law from the Dimension of the Data Manifold or the second half of this Welch Labs video).\nLess Canonical Hypotheses # The following hypotheses are either newer or less popular than the ones above, but I still think they\u0026rsquo;re all quite motivated and perhaps more relevant or interesting to think about today.\nThe Platonic Representation Hypothesis # This is quite a cool one:\nNeural networks, trained with different objectives on different data and modalities, are converging to a shared statistical model of reality in their representation spaces.\nPlato has finally been vindicated: different neural networks seem to be converging on the Platonic Forms. Find the above quote and original paper here.\nData over Algorithms # I used to think that AI progress was mostly about the compute and the algorithms. But there are some reasons why I no longer believe this (in no particular order):\nJack Morris has a popular tweet about the Pretraining Without Attention paper. He argues that based on the paper, if there are enough parameters in your neural network and it is \u0026ldquo;well-conditioned\u0026rdquo; with nonlinearities and connections, the exact architectural details may not matter. James Betkar argued in his blog (in June 2023!) the importance of data: \u0026ldquo;trained on the same dataset for long enough, pretty much every model with enough weights and training time converges to the same point.\u0026rdquo; Qwen3 and Llama 2 use very similar architectures. Qwen3 is a much more performant model (and the same goes for Llama 3). Filtering pretraining data to remove harmful knowledge works as you\u0026rsquo;d expect. Filtering pretraining data to remove mentions of AI misalignment reduces misalignment. Meta bought a 49% stake in Scale AI\u0026mdash;a data labeling company\u0026mdash;for over $14 billion. In my view, all these points suggest that models are mostly their data (given sufficient compute and an architecure that\u0026rsquo;s at least good enough) and that the frontier labs have known this for a while.\nOthers? # If you know of any other cool neural network hypotheses to keep an eye on, please shoot me an email or send a pigeon.\nOkay, technically this isn\u0026rsquo;t a neural network hypothesis, but it does have implications for high-dimensional models.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"January 24, 2026","externalUrl":null,"permalink":"/posts/neural_network_hypotheses/","section":"posts","summary":"From ancient Greece to manifolds.","title":"Various Neural Network Hypotheses","type":"posts"},{"content":" Near-Orthogonality # We\u0026rsquo;ll say that two vectors $v_1$ and $v_{2}$ are $\\varepsilon$-near orthogonal if $|\\langle v_1, v_2 \\rangle| \\leq \\varepsilon$. We then claim that there exists a set of $2^{\\Theta(\\varepsilon^2 d)}$ random vectors in $\\mathbb{R}^d$ such that all pairs of vectors in the set are nearly orthogonal with high probability.\nThe proof of the above claim isn\u0026rsquo;t too difficult. Let $v_1, \\dots, v_t \\in \\mathbb{R}^{d}$ each have independent random entries in $\\big\\{1/\\sqrt{d}, -1/\\sqrt{d}\\big\\}$. We\u0026rsquo;re interested in the probability that all pairs of these vectors are nearly orthogonal, or $\\Pr \\left[\\forall i, j, | \\langle v_{i}, v_{j} \\rangle | \u0026lt; \\varepsilon \\right]$.\nFirst, we notice that for any $v_{i}$, $\\lVert v_{i} \\rVert_{2} = 1$, and for any $i, j \\in [t]$ (for $i \\neq j$), \\[ \\text{E} \\left[\\langle v_{i}, v_{j} \\rangle \\right] = \\text{E} \\left[v_{i_{1}}\\right] \\text{E} \\left[v_{j_{1}}\\right] + \\dots + \\text{E} \\left[v_{i_{d}}\\right] \\text{E} \\left[v_{j_{d}}\\right] = 0. \\] But $v_{i_{\\ell}} \\cdot v_{j_{\\ell}}$ equals $1/d$ with probability 0.5, and $-1/d$ with probability 0.5. Therefore, we can define \\[ X_{\\ell} := \\frac{1 - d(v_{i_{\\ell}}v_{j_{\\ell}})}{2} \\sim \\text{Bern}(1/2), \\] and $Z := X_1 + \\dots + X_{t} \\sim \\text{Binomial}(t, 1/2)$.\nWe can now focus on bounding $Z$ instead of $| \\langle v_{i}, v_{j} \\rangle|$: \\[ \\begin{aligned} \\Pr \\left[|\\langle v_{i}, v_{j} \\rangle | \\geq \\varepsilon \\right] \u0026= \\Pr \\left[|Z| \\geq \\varepsilon \\right] \\\\ \u0026= \\Pr \\left[\\left|Z - \\frac{d}{2}\\right| \\geq \\varepsilon \\cdot \\frac{d}{2}\\right] \\\\ \u0026\\leq 2 \\exp (-\\varepsilon^{2}d/6), \\end{aligned} \\] where the last inequality is by a Chernoff bound. Applying this inequality to all $i \\neq j$ pairs and taking a union bound gets us: \\[ \\begin{aligned} \\Pr \\left[\\forall i \\neq j, | \\langle v_{i}, v_{j} \\rangle | \\geq \\varepsilon \\right] \u0026\\leq \\binom{t}{2} 2 \\exp (-\\varepsilon^{2}d/6) \\\\ \u0026\\leq \\frac{t^{2}}{2} 2 \\exp (-\\varepsilon^{2}d/6) \\\\ \u0026= t^{2} \\exp(\\varepsilon^{2}d/6) \\end{aligned} \\]We can take the compliment to get our desired inequality: \\[ \\Pr \\left[\\forall i \\neq j, | \\langle v_{i}, v_{j} \\rangle | \u003c \\varepsilon \\right] \\geq 1 - t^{2} \\exp(-\\varepsilon^{2}d/6). \\] Therefore, for $t$ satisfying\n\\[ \\begin{aligned} \u0026\\frac{t^{2}}{2} 2 \\exp(\\varepsilon^{2}d/6) \u003c 1 \\\\ \\Rightarrow \u0026 \\ t \u003c \\exp(\\varepsilon^{2}d/6) \\approx 2^{\\Theta(\\varepsilon^{2}d)}, \\end{aligned} \\] we have our $\\varepsilon$-near orthogonality. $\\blacksquare$\nWhy does this matter? # The above fact means that if we\u0026rsquo;re willing to let pairs of vectors be only $\\varepsilon$-near orthogonal in some $d$-dimensional space, we can actually fit an exponential number of vectors in that merely $d$-dimensional space. I first learned this fact from 3Blue1Brown\u0026rsquo;s transformer interpretability video, although both he and Anthropic attribute the fact to the Johnson-Lindenstrauss Lemma, which isn\u0026rsquo;t actually necessary for the proof, as we can see above.\nFor LLMs, this implies that despite having hidden sizes in the thousands or ten thousands, they can fit a contain a whole lot more linearly-encoded concepts in their weight space. As a quick example, if we have a hidden size of 8192 and put our epsilon threshold at $\\varepsilon = 0.1$, we can fit \\[ 2^{0.1^{2} \\cdot 8192} \\approx 4.57 \\cdot 10^{24} \\] $\\varepsilon$-near orthogonal vectors in the hidden space. That is a lot of concepts!\n","date":"January 16, 2026","externalUrl":null,"permalink":"/posts/linear_representation_math/","section":"posts","summary":"Hint: you don\u0026rsquo;t need Johnson-Lindenstrauss.","title":"Why does the linear representation hypothesis work?","type":"posts"},{"content":"The University of Chicago has a tradition dubbed the \u0026ldquo;Aims of Education Address\u0026rdquo;. As put by the university:\nEvery year since 1961, a University of Chicago faculty member has been invited to address students in the College regarding their view on the aims of a liberal education. In 1962 the Aims of Education Address was added to Orientation Week and officially became a tradition for incoming students. The address encourages students to reflect on the purpose and definition of education as they embark upon their collegiate years.\nIn 2022, the speaker at the Aims of Education was Agnes Callard. She\u0026rsquo;s a relatively prominent figure in philosophical academia, so I recently decided to watch her address. Interestingly, around 11 minutes into her address, she starts talking about longtermism, which is one of the key beliefs in effective altruist and rationalist circles. I\u0026rsquo;ve transcribed the most relevant part of the speech below. (The following paragraphs are all taken from Agnes Callard\u0026rsquo;s address, linked above.)\n(Part of) The Address # \u0026ldquo;So recently I read a book called What We Owe the Future, which tries to argue the reader into longtermism, which is the view that we ought to care about the future. I was surprised that the author William McKascal, who is a philosopher, at no point in the book whipped out the philosophy thought experiment designed to show that we already do. Why go to all the trouble trying to browbeat someone into caring about something when instead you can show them they already do care about it? Myself, I like to take the easy path. So, I\u0026rsquo;m going to tell you about that thought experiment.\nSuppose we find out tomorrow that over the past few months, another virus alongside COVID has been creeping silently around the globe. The disease caused by this highly infectious virus is largely asymptomatic, which is why it took us a few months to notice that just about everyone has been infected by it. In fact, assume that by the time we figure out exactly what the virus does, everyone on the planet has been infected. [\u0026hellip;] I\u0026rsquo;m also going to posit what the virus does. It makes you infertile.\nThe virus gets called Sterilla virus because its only medical effect on those human it infects—whether they\u0026rsquo;re male or female—is sterility, the inability to have children. What I want you to think about is what would be your reaction to learning that an epidemic of Sterilla virus has swept the globe? While you\u0026rsquo;re thinking about that, I\u0026rsquo;ll tell you about my reaction. [\u0026hellip;] My reaction is that I feel sick.\nI\u0026rsquo;m borrowing the outlines of this thought experiment, including the label, the \u0026ldquo;infertility scenario\u0026rdquo;, from philosopher Samuel Scheffler\u0026rsquo;s book, Death and the Afterlife, though I\u0026rsquo;ve modified it a little. Scheffler is himself drawing on the P.D. James novel, Children of Men, which Alfonso Quiron made into a movie with Clive Owen in 2006. The movie differs from the novel in a lot of details. Both are good, but something they share is a focus on just how dystopian our world would get in the decades before our time ran out. Everyone, but especially the last people to be born who are called omegas in the book seems to have lost the ability to care. Scheffler points out this is surprising. Each of us knows we\u0026rsquo;re going to die, yet we take this in stride and live productive lives. Somehow, when we\u0026rsquo;re confronted not even with the deaths, but just the nonbirths of future people (which is to say people who could have existed but didn\u0026rsquo;t), it\u0026rsquo;s in the face of this weird metaphysical lacuna that our that our ability to cope somehow fails us.\nIt wouldn\u0026rsquo;t be surprising if we were upset by the prospect of a meteor coming and killing all of us, because that would cut our lives short and some of us would die in great suffering. In the infertility scenario, no one you know or love is dying prematurely. And yet, P.D. James and Alfonso Quaron seem to think that in the period after humanity learns that they\u0026rsquo;re in the infertility scenario, but before they actually go extinct, human life on Earth would become a living hell. Scheffler conjectures that such a world would feature \u0026ldquo;widespread apathy, enemy and despair, the erosion of social institutions and social solidarity, the deterioration of the physical environment, a pervasive loss of conviction about the value or point of many activities.\u0026rdquo; My gut tells me he\u0026rsquo;s right.\nBut why? Why do future generations matter so much?\nScheffler\u0026rsquo;s explanation, which I find plausible, is that absent the prospect of future generations, human life seems to lose its meaning. That is, the meaning of our lives now relies in ways we don\u0026rsquo;t always notice or take into account on the existence of future lives. Scheffler says, \u0026ldquo;The coming into existence of people we do not know and love matters more to us than our own survival and the survival of the people we do know and love.\u0026rdquo; He thinks in the infertility scenario, the time after death becomes \u0026ldquo;a blank eternity of non-existence.\u0026rdquo; But what is it now? Well, he compares it to a party we have to leave early. His thought is that a lot of the most meaningful activities that we engage in are parts of ongoing traditions. It matters to us that the party—be it about scientific achievement, political struggles, religious observance, literary appreciation—not come to an end with our contribution to it.\nHere\u0026rsquo;s another quote from Scheffler: \u0026ldquo;Our conception of a human life as a whole relies on an implicit understanding of such a life as itself occupying a place in an ongoing history uh in a temporally extended chain of lives and generations.\u0026rdquo; If this is so, then perhaps we cannot simply take it for granted that the activity of say reading The Cather in the Rye or trying to understand quantum mechanics or even eating an excellent meal would have the same significance for people or offer them the same rewards in a world that was known to be deprived of a human future.\nWhat I notice is that my response to the infertility scenario is very different from one in which from a scenario in which I simply don\u0026rsquo;t have biological descendants. If I were to learn that none of my children were going to have children, I might be a bit saddened. (Please have kids, guys!) But I don\u0026rsquo;t feel that vertigenous1 loss of meaning. I wouldn\u0026rsquo;t even feel it if I learned that no one I currently knew would have any descendants alive in a 100 years as long as other humans did have descendants. It looks like I don\u0026rsquo;t especially care that I survive or that I or my associates leave a chain of descendants behind us. What I seem to care about is a set of people who haven\u0026rsquo;t been born yet, who I have no personal connection to: the humans of 2200 to 5000.\u0026rdquo;\nMy Reaction # I find this argument (which I was unaware of before watching this address) very compelling. To be honest, I\u0026rsquo;m not sure why it isn\u0026rsquo;t brought up more in longtermist circles2. The rest of the talk is fairly good as well (although I disagree that the fact that Scheffler\u0026rsquo;s \u0026ldquo;pyramid scheme\u0026rdquo; proposal for how humans define meaning is invalid because it is illogical; I believe that the validity of an argument has little to do with the extent to which that argument influences our lives).\nGreat word.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOkay, well I know why it isn\u0026rsquo;t brought up: it makes the longtermist ideology seem largely unnecessary due to its intrinsic existence in humans.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"September 26, 2025","externalUrl":null,"permalink":"/posts/agnes_callard_longtermism/","section":"posts","summary":"Should longtermists bother advocating for longtermism?","title":"Agnes Callard on Longtermism","type":"posts"},{"content":" Publications # Sanderson, J., Foley, T., Guo, S., Qu, A., \u0026amp; Josephson, H. (2025). Rethinking LLM advancement: Compute-dependent and independent paths to progress. https://arxiv.org/abs/2505.04075\n","externalUrl":null,"permalink":"/publications/","section":"Jack Sanderson","summary":"","title":"","type":"page"},{"content":"This site is ever evolving (about once per month). I want it to be minimal, fast, and user-friendly. The design is largely inspired by the personal website of Alexander Paulin and partially by Henry Josephson and Daniel Paleka.\nHere are some facts about me:\nI\u0026rsquo;m a big fan of the Chicago Bears and Cubs I play the ukulele I\u0026rsquo;m an avid neovim user I\u0026rsquo;m probably best described as an anti-rationalist effective altruism sympathizer (though not apologist) (source) If you have a fun fact you\u0026rsquo;d like me to put here, shoot me an email!\n","externalUrl":null,"permalink":"/about/","section":"Jack Sanderson","summary":"","title":"about","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"Discrete Math Discrete math at UChicago (CMSC 27100) was the first proof-based class I took. It definitely was very new at first, so I compiled my notes in an effort to review.\nDiscrete Notes\n","externalUrl":null,"permalink":"/courses/","section":"Jack Sanderson","summary":"","title":"courses","type":"page"},{"content":"I am trying my best this summer to become more productive. I\u0026rsquo;m not a serial doomscroller or anything, but I like small, incremental self-improvement. I figure I\u0026rsquo;m going to start keeping a timeline of my systems and routines to see how things change over time (some of this will be backfilled; I started this in July of 2025).\ntimeline # Any date that doesn\u0026rsquo;t have an end means I\u0026rsquo;m still using the tool/system/etc.\nLeechblock (November 2025) Inspired by Matt Might, I\u0026rsquo;m now using Leechblock to block all social media on my computer for the majority of the day. It\u0026rsquo;s definitely annoying, but helps me stay on task quite well. Daily Markdown File (October 2025) After being tired of not having a place to collect all my random thoughts, I created a simple vim remap (\u0026lt;leader\u0026gt;tod) to take me to a daily.md file and dump any idea/long-term todo I have. (I call it daily.md because I can insert a daily heading with \u0026lt;leader\u0026gt;date!) So far it seems mildly useful, though I haven\u0026rsquo;t been using it every day. Doing so is my next goal. VimTeX (September 2025) I setup VimTeX in the style of Gilles Castelle following Elijan Mastnak\u0026rsquo;s guide. There\u0026rsquo;s a pretty strong learning curve, but it\u0026rsquo;s great once you start to memorize your snippets.\nUpdate January 2026: I initially used the Skim PDF reader but I found it annoying as when the PDF was in the \u0026ldquo;single-page continuous\u0026rdquo; view mode, it would flash black upon the LaTeX re-rendering on save. I just switched to Sioyek to avoid this issue and highly recommend it.\nFlashSpace (July 2025) I am now using FlashSpace to manage spaces. I initially set up Aerospace but found it a bit clunky for my liking. I\u0026rsquo;m more of a one screen, one task guy, so I didn\u0026rsquo;t care at all about tiling and just wanted the space control. FlashSpace does this faster and more efficiently. Social Media Blocking (July 2025) It was becoming way to easy for me to just open twitter or reddit and doomscroll, so I\u0026rsquo;ve setup ublock to block the home screen for both of these apps (also youtube). I can still search for a youtube video or specific subreddits. Grayscale Phone (June 2025) Right when school ended for the 2024-2025 academic year, I finally committed to grayscaling my phone. At first it definitely made using it more appealing, but I think the only reason I\u0026rsquo;ve kept it around is because now the full color mode has become straining to look at. (The purpose of doing this right as school ended was to promote a summer lock-in, and I think I mostly succeeded!) Mapping Ctrl \u0026#43; H/L to Mac Spaces Navigation (June 2025–July 2025)) This was because I was sick of using the trackpad to move between spaces. It was still a bit too slow for me, which is why I\u0026rsquo;ve since switched to FlashSpace. Todoist (March 2025) I had used Todoist for a bit before, but I tried to make it very low friction to use and it finally stuck this try. I absolutely love the natural language syntax. ScreenZen (September 2024) This is probably the single best decision I\u0026rsquo;ve ever made. Thank you Drew Gooden. My limit is 5 opens/app, 7 minutes each (for all social media).\nAfter a year, I usually don\u0026rsquo;t even reach 1 open for each app.\nNeovim (May 2024) After using vanilla vim for a few months, neovim was a nice upgrade. I still use the same base (kickstart) config to this day. I try my best to keep my config files clean and keep my config minimal. Vim (March 2024–May 2024)) It was recommended by my professor for Systems Programming I. Glad I stuck with it. Meditation (January–March 2024) I started meditating daily in January of 2024, but stopped by March. Don\u0026rsquo;t have a great explanation for why other than it became less of a priority. (Vim became my meditation.) ","externalUrl":null,"permalink":"/productivity/","section":"Jack Sanderson","summary":"","title":"productivity","type":"page"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]