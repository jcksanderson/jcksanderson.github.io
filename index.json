[{"content":" Publications # Sanderson, J., Foley, T., Guo, S., Qu, A., \u0026amp; Josephson, H. (2025). Rethinking LLM advancement: Compute-dependent and independent paths to progress. https://arxiv.org/abs/2505.04075\n","externalUrl":null,"permalink":"/publications/","section":"Jack Sanderson","summary":"","title":"","type":"page"},{"content":"This site is ever evolving (about once per month). Here are some facts about me:\nI\u0026rsquo;m a big fan of the Chicago Bears and Cubs I play the ukulele I\u0026rsquo;m an avid neovim user If you have a fun fact you\u0026rsquo;d like me to put here, shoot me an email!\nHere\u0026rsquo;s a beautiful poem of mine about global warming:\nsee low c o o 2 oh no see high sea rise oh me oh my we hide disguise but fate won’t lie ","externalUrl":null,"permalink":"/about/","section":"Jack Sanderson","summary":"","title":"about","type":"page"},{"content":"The University of Chicago has a tradition dubbed the \u0026ldquo;Aims of Education Address\u0026rdquo;. As put by the university:\nEvery year since 1961, a University of Chicago faculty member has been invited to address students in the College regarding their view on the aims of a liberal education. In 1962 the Aims of Education Address was added to Orientation Week and officially became a tradition for incoming students. The address encourages students to reflect on the purpose and definition of education as they embark upon their collegiate years.\nIn 2022, the speaker at the Aims of Education was Agnes Callard. She\u0026rsquo;s a relatively prominent figure in philosophical academia, so I recently decided to watch her address. Interestingly, around 11 minutes into her address, she starts talking about longtermism, which is one of the key beliefs in effective altruist and rationalist circles. I\u0026rsquo;ve transcribed the most relevant part of the speech below. (The following paragraphs are all taken from Agnes Callard\u0026rsquo;s address, linked above.)\n(Part of) The Address # \u0026ldquo;So recently I read a book called What We Owe the Future, which tries to argue the reader into longtermism, which is the view that we ought to care about the future. I was surprised that the author William McKascal, who is a philosopher, at no point in the book whipped out the philosophy thought experiment designed to show that we already do. Why go to all the trouble trying to browbeat someone into caring about something when instead you can show them they already do care about it? Myself, I like to take the easy path. So, I\u0026rsquo;m going to tell you about that thought experiment.\nSuppose we find out tomorrow that over the past few months, another virus alongside COVID has been creeping silently around the globe. The disease caused by this highly infectious virus is largely asymptomatic, which is why it took us a few months to notice that just about everyone has been infected by it. In fact, assume that by the time we figure out exactly what the virus does, everyone on the planet has been infected. [\u0026hellip;] I\u0026rsquo;m also going to posit what the virus does. It makes you infertile.\nThe virus gets called Sterilla virus because its only medical effect on those human it infects—whether they\u0026rsquo;re male or female—is sterility, the inability to have children. What I want you to think about is what would be your reaction to learning that an epidemic of Sterilla virus has swept the globe? While you\u0026rsquo;re thinking about that, I\u0026rsquo;ll tell you about my reaction. [\u0026hellip;] My reaction is that I feel sick.\nI\u0026rsquo;m borrowing the outlines of this thought experiment, including the label, the \u0026ldquo;infertility scenario\u0026rdquo;, from philosopher Samuel Scheffler\u0026rsquo;s book, Death and the Afterlife, though I\u0026rsquo;ve modified it a little. Scheffler is himself drawing on the P.D. James novel, Children of Men, which Alfonso Quiron made into a movie with Clive Owen in 2006. The movie differs from the novel in a lot of details. Both are good, but something they share is a focus on just how dystopian our world would get in the decades before our time ran out. Everyone, but especially the last people to be born who are called omegas in the book seems to have lost the ability to care. Scheffler points out this is surprising. Each of us knows we\u0026rsquo;re going to die, yet we take this in stride and live productive lives. Somehow, when we\u0026rsquo;re confronted not even with the deaths, but just the nonbirths of future people (which is to say people who could have existed but didn\u0026rsquo;t), it\u0026rsquo;s in the face of this weird metaphysical lacuna that our that our ability to cope somehow fails us.\nIt wouldn\u0026rsquo;t be surprising if we were upset by the prospect of a meteor coming and killing all of us, because that would cut our lives short and some of us would die in great suffering. In the infertility scenario, no one you know or love is dying prematurely. And yet, P.D. James and Alfonso Quaron seem to think that in the period after humanity learns that they\u0026rsquo;re in the infertility scenario, but before they actually go extinct, human life on Earth would become a living hell. Scheffler conjectures that such a world would feature \u0026ldquo;widespread apathy, enemy and despair, the erosion of social institutions and social solidarity, the deterioration of the physical environment, a pervasive loss of conviction about the value or point of many activities.\u0026rdquo; My gut tells me he\u0026rsquo;s right.\nBut why? Why do future generations matter so much?\nScheffler\u0026rsquo;s explanation, which I find plausible, is that absent the prospect of future generations, human life seems to lose its meaning. That is, the meaning of our lives now relies in ways we don\u0026rsquo;t always notice or take into account on the existence of future lives. Scheffler says, \u0026ldquo;The coming into existence of people we do not know and love matters more to us than our own survival and the survival of the people we do know and love.\u0026rdquo; He thinks in the infertility scenario, the time after death becomes \u0026ldquo;a blank eternity of non-existence.\u0026rdquo; But what is it now? Well, he compares it to a party we have to leave early. His thought is that a lot of the most meaningful activities that we engage in are parts of ongoing traditions. It matters to us that the party—be it about scientific achievement, political struggles, religious observance, literary appreciation—not come to an end with our contribution to it.\nHere\u0026rsquo;s another quote from Scheffler: \u0026ldquo;Our conception of a human life as a whole relies on an implicit understanding of such a life as itself occupying a place in an ongoing history uh in a temporally extended chain of lives and generations.\u0026rdquo; If this is so, then perhaps we cannot simply take it for granted that the activity of say reading The Cather in the Rye or trying to understand quantum mechanics or even eating an excellent meal would have the same significance for people or offer them the same rewards in a world that was known to be deprived of a human future.\nWhat I notice is that my response to the infertility scenario is very different from one in which from a scenario in which I simply don\u0026rsquo;t have biological descendants. If I were to learn that none of my children were going to have children, I might be a bit saddened. (Please have kids, guys!) But I don\u0026rsquo;t feel that vertigenous1 loss of meaning. I wouldn\u0026rsquo;t even feel it if I learned that no one I currently knew would have any descendants alive in a 100 years as long as other humans did have descendants. It looks like I don\u0026rsquo;t especially care that I survive or that I or my associates leave a chain of descendants behind us. What I seem to care about is a set of people who haven\u0026rsquo;t been born yet, who I have no personal connection to: the humans of 2200 to 5000.\u0026rdquo;\nMy Reaction # I find this argument (which I was unaware of before watching this address) very compelling. To be honest, I\u0026rsquo;m not sure why it isn\u0026rsquo;t brought up more in longtermist circles2. The rest of the talk is fairly good as well (although I disagree that the fact that Scheffler\u0026rsquo;s \u0026ldquo;pyramid scheme\u0026rdquo; proposal for how humans define meaning is invalid because it is illogical; I believe that the validity of an argument has little to do with the extent to which that argument influences our lives).\nGreat word.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOkay, well I know why it isn\u0026rsquo;t brought up: it makes the longtermist ideology seem largely unnecessary due to its intrinsic existence in humans.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","externalUrl":null,"permalink":"/posts/agnes_callard_longtermism/","section":"posts","summary":"Should longtermists bother advocating for longtermism?","title":"Agnes Callard on Longtermism","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"Discrete Math Discrete math at UChicago (CMSC 27100) was the first proof-based class I took. It definitely was very new at first, so I compiled my notes in an effort to review.\nDiscrete Notes\n","externalUrl":null,"permalink":"/courses/","section":"Jack Sanderson","summary":"","title":"courses","type":"page"},{"content":"Hi! My name is Jack and I\u0026rsquo;m a third-year undergraduate at the University of Chicago studying computer science and statistics. I\u0026rsquo;m largely interested in AI security, including model tampering, backdooring, and data poisoning. This summer I created an AI Security Course along with some other undergrads at UChicago. I\u0026rsquo;m also involved in the field of AI safety (although I reject many of the field\u0026rsquo;s most common views). In the past, I did investigated compute-algorithm interactions, which was featured on Epoch AI\u0026rsquo;s Gradient Updates. I\u0026rsquo;m best reached over email.\n","externalUrl":null,"permalink":"/","section":"Jack Sanderson","summary":"","title":"Jack Sanderson","type":"page"},{"content":" Setting the Scene # Objective functions (AKA loss functions, cost functions, utility functions, etc.) are the guiding light behind machine learning. Without them, algorithms would be groping in the dark, which definitely isn\u0026rsquo;t what we want (it would lead to quite bad ML models and be a PR nightmare).\nThe discussion of objective functions here is especially pertinent because of the AI alignment problem coming out of the field of AI safety, an area I\u0026rsquo;m inextricably involved in. Put simply, the AI alignment problem states that we currently have no means of assuring that the values of an AI system align with those of humanity. This, according to AI x-risk enthusiasts1, is quite concerning; it means that if we were to produce a superintelligent AI, it very well might want to take over the universe and learn to view humanity as a crutch, ultimately deciding to eradicate us all (except for me, because I am really cool and likable, and of course you the reader as well, because you are also very cool and likable).\nThis would, of course, be tragic, as without the rest of humanity, there\u0026rsquo;d be nobody left to notice how cool and likable you and I are. Also, I want to note that I do not that I don\u0026rsquo;t especially buy into the AI alignment problem, for reasons the government may or may not be forbidding me from getting into2. But I digress. I find it interesting that objective functions don\u0026rsquo;t come up as much in discussions surrounding AI alignment.\nProposal: if we can just get the objective function just right, we\u0026rsquo;ll be able to guarantee that the model can only move towards being aligned with humans. Why doesn\u0026rsquo;t this proposal work? Basically, it\u0026rsquo;s not as straightforward as getting the reward function \u0026ldquo;just right.\u0026rdquo;\nTake reward hacking. Put simply, reward hacking is an alignment failure wherein a reinforcement learning agent—through policy optimization—discovers and exploits a misspecified proxy reward function to maximize its expected return, thereby converging on a degenerate policy that is optimal for the given reward signal but misaligned with the designer\u0026rsquo;s latent objective. A bit more technically, reward hacking is when an agent finds a clever loophole to get the maximum reward without actually accomplishing the real goal. The underlying problem is that humans tend to design reward functions that are intuitively good proxies for ideal behavior, but contain corner cases that leave them susceptible to exploitation. Thus, one might be inclined to call for the creation of a science of objective functions, a rigorous methodology to better define for machines what our goals are. I do not call for this.\nWhy an \u0026ldquo;art\u0026rdquo;—not science—of objective functions? # \u0026ldquo;Learn the rules like a pro, so you can break them like an artist. — Pablo Picasso\u0026rdquo; — Jack Sanderson (alright, maybe not)\nI don\u0026rsquo;t think there\u0026rsquo;s much to say about the \u0026ldquo;science\u0026rdquo; of objective functions. In my view, most of the current work on objectives is already scientific, driven more by theory than beauty, but mostly by empirical results. It\u0026rsquo;s time we brought in some aesthetics from information theory to make this discipline more attractive.\nDespite my early introduction of reward hacking, I don\u0026rsquo;t actually care about it; I leave solving reward hacking as an exercise for the objective function scientists of the world. Concretely, what I\u0026rsquo;m interseted in is a) the information-theoretic intuition behind objective functions, and b) the outcomes we should expect given these intuitions.\nThe Archetypal Example # Say we\u0026rsquo;re calculating the loss of a language model (LM) over a sequence of tokens \\(x_{i:n}\\). We typically do this with the cross-entropy loss (equivalent to negative log-likelihood) as given here: \\[ \\mathcal{L}(x_{1:n}) = - \\sum_{i = 1}^n \\log p(x_{i} | x_{1:i}) \\] In English, the LM\u0026rsquo;s loss over the whole sequence is equal to the negative log-probability of token \\(i\\) givne all tokens up to \\(i\\) as input. Why negative log-likelihood? Consider its graph on the interval \\((0, 1]\\): Hopefully, the graph makes clear why negative log-likelihood is a great loss function. As our input \\(x = p(x_i | x_{1:i})\\) gets closer to \\(1\\), the loss approaches \\(0\\), meaning the higher the probability our LM assigns the correct token, the less it\u0026rsquo;ll be penalized. On the other hand, as \\(x\\) gets closer to \\(0\\), the loss blows up, asymptotically approaching infinity. It is for this reason that LLMs are often dubbed \u0026ldquo;next-token predictors,\u0026rdquo; as the base LM indeed is only trained on next-token prediction (NTP).\nWhat can we glean from this objective function? Well, one reasonable conclusion is that a powerful LM trained purely on NTP (this is often called a base model or pretrained model) is going to be good at modeling language. Given some input tokens, it will skillfully be able to produce new tokens that are highly reasonable when conditioning on the input. On the other hand, there\u0026rsquo;s absolutely no reason we should expect this model to be good at conversing with users. Say we prompt it with a question:\nQ: What is the capital of France? It wouldn\u0026rsquo;t be unreasonable for the base LM to begin its completion with either of these two strings:\nA1: The capital of France is Paris. A2: What is the capital of Germany? Why do I claim this? Because these models are trained for NTP over the entire internet, which likely contains both pairs of questions and answers and long lists of questions. (If you\u0026rsquo;d like to go a bit deeper into the world of pre-training and instruction-tuning, I recommend this article, which also contains some funny completions straight from a base model.)\nThe point here is that just from the objective function, we can start to reason about how we expected a model trained on such an objective will act. The goal of this series is to understand the interplay of these objectives in multi-stage training processes (and dig into some more complicated objective functions?).\nhttps://x.com/ESYudkowsky/status/1968353926035783830\nNot that they\u0026rsquo;re enthusiastic about the notion of AI-induced existential risk, but that they\u0026rsquo;re especially interested in mitigating it.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAlright, maybe it\u0026rsquo;s not the \u0026ldquo;may\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","externalUrl":null,"permalink":"/posts/objective_functions_intro/","section":"posts","summary":"What doing we care about them more than we do?","title":"Objective Functions","type":"posts"},{"content":"","externalUrl":null,"permalink":"/series/objectives/","section":"Series","summary":"","title":"Objectives","type":"series"},{"content":"Some writing.\n","externalUrl":null,"permalink":"/posts/","section":"posts","summary":"","title":"posts","type":"posts"},{"content":"I am trying my best this summer to become more productive. I\u0026rsquo;m not a serial doomscroller or anything, but I like small, incremental self-improvement. I figure I\u0026rsquo;m going to start keeping a timeline of my systems and routines to see how things change over time (some of this will be backfilled; I started this in July of 2025).\ntimeline # Any date that doesn\u0026rsquo;t have an end means I\u0026rsquo;m still using the tool/system/etc.\nFlashSpace (July 2025) I am now using FlashSpace to manage spaces. I initially set up Aerospace but found it a bit clunky for my liking. I\u0026rsquo;m more of a one screen, one task guy, so I didn\u0026rsquo;t care at all about tiling and just wanted the space control. FlashSpace does this faster and more efficiently. Social Media Blocking (July 2025) It was becoming way to easy for me to just open twitter or reddit and doomscroll, so I\u0026rsquo;ve setup ublock to block the home screen for both of these apps (also youtube). I can still search for a youtube video or specific subreddits. Mapping Ctrl \u0026#43; H/L to Mac Spaces Navigation (June 2025–July 2025)) This was because I was sick of using the trackpad to move between spaces.\nIt was still a bit too slow for me, which is why I\u0026rsquo;ve since switched to FlashSpace.\nTodoist (March 2025) I had used Todoist for a bit before, but I tried to make it very low friction to use and it finally stuck this try. I absolutely love the natural language syntax. ScreenZen (September 2024) This is probably the single best decision I\u0026rsquo;ve ever made. Thank you Drew Gooden. My limit is 5 opens/app, 7 minutes each (for all social media).\nAfter a year, I usually don\u0026rsquo;t even reach 1 open for each app.\nNeovim (May 2024) After using vanilla vim for a few months, neovim was a nice upgrade. I still use the same base (kickstart) config to this day. I try my best to keep my config files clean and keep my config minimal. Vim (March 2024–May 2024)) It was recommended by my professor for Systems Programming I. Glad I stuck with it. Meditation (January–March 2024) I started meditating daily in January of 2024, but stopped by March. Don\u0026rsquo;t have a great explanation for why other than it became less of a priority. (Vim became my meditation.) ","externalUrl":null,"permalink":"/productivity/","section":"Jack Sanderson","summary":"","title":"productivity","type":"page"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]